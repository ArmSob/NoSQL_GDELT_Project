{"paragraphs":[{"text":"import sys.process._\nimport java.net.{URL, HttpURLConnection}\nimport java.nio.file.{Files,StandardCopyOption}\nimport java.io.File\n\nimport org.apache.spark.sql.types\nimport org.apache.spark.sql.Column\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.input.PortableDataStream\nimport java.util.zip.ZipInputStream\nimport java.io.BufferedReader\nimport java.io.InputStreamReader\n\nimport com.amazonaws.services.s3.AmazonS3Client\nimport com.amazonaws.auth.BasicAWSCredentials\nimport com.amazonaws.auth.BasicSessionCredentials","user":"anonymous","dateUpdated":"2021-01-18T16:55:18+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"import sys.process._\nimport java.net.{URL, HttpURLConnection}\nimport java.nio.file.{Files, StandardCopyOption}\nimport java.io.File\nimport org.apache.spark.sql.types\nimport org.apache.spark.sql.Column\nimport org.apache.spark.sql.functions._\nimport org.apache.spark.sql.SQLContext\nimport org.apache.spark.input.PortableDataStream\nimport java.util.zip.ZipInputStream\nimport java.io.BufferedReader\nimport java.io.InputStreamReader\nimport com.amazonaws.services.s3.AmazonS3Client\nimport com.amazonaws.auth.BasicAWSCredentials\nimport com.amazonaws.auth.BasicSessionCredentials\n"}]},"apps":[],"jobName":"paragraph_1610983576751_-1503137780","id":"20210108-143457_1719135930","dateCreated":"2021-01-18T15:26:16+0000","dateStarted":"2021-01-18T16:55:18+0000","dateFinished":"2021-01-18T16:55:19+0000","status":"FINISHED","progressUpdateIntervalMs":500,"focus":true,"$$hashKey":"object:11663"},{"text":"// S3 bucket access configuration\n    \nval AWS_ID = \"ASIAZLK42DVGQHFM2DXD\"\nval AWS_KEY = \"eYnd71EuMb1fDF6x54ZmF04J08WwT6JVLTBCDCn9\"\nval AWS_SESSION_TOKEN = \"FwoGZXIvYXdzEJr//////////wEaDIG3ec6yUChDWZiOWiLTAZjHN2icXIkoj/YHJ4l+cr2N4e9tWvJBE4dW+Q5phgJ86Qx97ieOtfc1YWnNQHNwNgk1/dFdITHrzJW2hnBkcHwELOjfecLYCO6BeVBzsJSdxKyfUNc3AQErcSRTEIHMlkGFpwzSkjxplMCNpE8L+sqKU/kXJpufoDj17eX8U/x36C88IpMevfaGBgcogpoFvDuNuBceG027/KOy+JOxp6xa/FMhTQLaCx/dpw5QYg7o3inKUWgSVc4WZBzU4P91G9tg7AY/R5rTTG73tbRnppnr7TYojvuWgAYyLdBucRPXBMrU5vqh7A0VkQAapOt4XeBaXZu3KGILunp/YyplkCV8BSBkLk5QYA==\"\n\n@transient val awsClient = new AmazonS3Client(new BasicSessionCredentials(AWS_ID, AWS_KEY, AWS_SESSION_TOKEN))\n\nsc.hadoopConfiguration.set(\"fs.s3a.access.key\", AWS_ID)\nsc.hadoopConfiguration.set(\"fs.s3a.secret.key\", AWS_KEY)\nsc.hadoopConfiguration.set(\"fs.s3a.secret.token\", AWS_SESSION_TOKEN)","user":"anonymous","dateUpdated":"2021-01-18T16:56:15+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"warning: there was one deprecation warning; re-run with -deprecation for details\nAWS_ID: String = ASIAZLK42DVGQHFM2DXD\nAWS_KEY: String = eYnd71EuMb1fDF6x54ZmF04J08WwT6JVLTBCDCn9\nAWS_SESSION_TOKEN: String = FwoGZXIvYXdzEJr//////////wEaDIG3ec6yUChDWZiOWiLTAZjHN2icXIkoj/YHJ4l+cr2N4e9tWvJBE4dW+Q5phgJ86Qx97ieOtfc1YWnNQHNwNgk1/dFdITHrzJW2hnBkcHwELOjfecLYCO6BeVBzsJSdxKyfUNc3AQErcSRTEIHMlkGFpwzSkjxplMCNpE8L+sqKU/kXJpufoDj17eX8U/x36C88IpMevfaGBgcogpoFvDuNuBceG027/KOy+JOxp6xa/FMhTQLaCx/dpw5QYg7o3inKUWgSVc4WZBzU4P91G9tg7AY/R5rTTG73tbRnppnr7TYojvuWgAYyLdBucRPXBMrU5vqh7A0VkQAapOt4XeBaXZu3KGILunp/YyplkCV8BSBkLk5QYA==\nawsClient: com.amazonaws.services.s3.AmazonS3Client = com.amazonaws.services.s3.AmazonS3Client@75fd12cf\n"}]},"apps":[],"jobName":"paragraph_1610983753394_-1554090944","id":"20210118-152913_1311267838","dateCreated":"2021-01-18T15:29:13+0000","dateStarted":"2021-01-18T16:56:15+0000","dateFinished":"2021-01-18T16:56:16+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11664"},{"text":"// Fonction permettant de télécharger un fichier à une URL donnée","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576751_-1410063592","id":"20210116-170656_513783295","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11665"},{"text":"def fileDownloader(urlOfFileToDownload: String, fileName: String) = {\r\n    val url = new URL(urlOfFileToDownload)\r\n    val connection = url.openConnection().asInstanceOf[HttpURLConnection]\r\n    connection.setConnectTimeout(5000)\r\n    connection.setReadTimeout(5000)\r\n    connection.connect()\r\n\r\n    if (connection.getResponseCode >= 400)\r\n        println(\"error\")\r\n    else\r\n        url #> new File(fileName) !!\r\n    }\r\n    ","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576752_-995514333","id":"20210108-143606_1933463046","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11666"},{"text":"// Téléchargement du fichier masterfilelist.txt et masterfilelist-translation.txt","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576752_-1207144752","id":"20210116-170722_590376783","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11667"},{"text":"fileDownloader(\"http://data.gdeltproject.org/gdeltv2/masterfilelist.txt\",\r\n \"/home/volume/Fichiers_gdelt/masterfilelist.txt\") // save the list file to the Spark Master","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576752_-431599159","id":"20210116-164420_953528503","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11668"},{"text":"fileDownloader(\"http://data.gdeltproject.org/gdeltv2/masterfilelist-translation.txt\",\r\n \"/home/volume/Fichiers_gdelt/masterfilelist-translation.txt\") // save the list file to the Spark Master","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576753_1729957772","id":"20210116-174611_1765183249","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11669"},{"text":"// Définition d'un dataframe ayant une colonne contenant les url des fichiers souhaités (tri sur un jour et une heure spécifique ici)","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576753_-669553774","id":"20210116-170745_942174608","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11670"},{"text":"val df = sqlContext.read\r\n    .option(\"delimiter\",\" \")\r\n    .csv(\"/home/volume/Fichiers_gdelt/masterfilelist.txt\")\r\n    .select(\"_c2\")\r\n    .filter(col(\"_c2\")\r\n    .contains(\"/20200615041500\"))\r\n\r\n\r\nval df2 = sqlContext.read\r\n    .option(\"delimiter\",\" \")\r\n    .csv(\"/home/volume/Fichiers_gdelt/masterfilelist-translation.txt\")\r\n    .select(\"_c2\")\r\n    .filter(col(\"_c2\")\r\n    .contains(\"/20200615041500\"))","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576754_-579870771","id":"20210112-084812_265459023","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11671"},{"text":"df.show(false)","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576754_-2145694673","id":"20210116-171033_741031834","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11672"},{"text":"df2.show(false)","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576754_1302155555","id":"20210116-175202_468973358","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11673"},{"text":"// Téléchargement des fichiers voulus","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576755_-897548316","id":"20210116-171720_2070157444","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11674"},{"text":"df.repartition(200).foreach( r=> {\r\n        val URL = r.getAs[String](0)\r\n        val fileName = r.getAs[String](0).split(\"/\").last\r\n        val dir = \"/home/volume/Fichiers_gdelt/20200615/\"\r\n        val localFileName = dir + fileName\r\n        fileDownloader(URL,  localFileName)\r\n        val localFile = new File(localFileName)\r\n     })","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576755_-816003653","id":"20210108-155537_791854391","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11675"},{"text":"df2.repartition(200).foreach( r=> {\r\n        val URL = r.getAs[String](0)\r\n        val fileName = r.getAs[String](0).split(\"/\").last\r\n        val dir = \"/home/volume/Fichiers_gdelt/20200615/\"\r\n        val localFileName = dir + fileName\r\n        fileDownloader(URL,  localFileName)\r\n        val localFile = new File(localFileName)\r\n     })","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576756_378167173","id":"20210116-175106_1188472795","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11676"},{"text":"\n\n\n// Points difficiles : Obtenir un RDD d'un ensemble de fichiers Zip. Une fois obtenu, on transforme les lignes en un unique Array de string via le split sur tab (\"\\t\"). La fonction split ne pouvant pas se faire sur un Dataframe.\n//                     On transforme ce RDD en Dataframe et une fois obtenu, on peut créer un second Dataframe ayant plusieurs colonnes en choisissant les éléments au sein de l'array du Dataframe initial.","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576756_1097310890","id":"20210116-164114_33209133","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11677"},{"text":"// MENTIONS : Données en anglais + translingual\r\nval Mentions_RDD = sc.binaryFiles(\"s3://vincent-partimbene-telecom-gdelt2020//20200615*.mentions.CSV.zip\")\r\n    .flatMap { case (name: String, content: PortableDataStream) =>\r\n    val zis = new ZipInputStream(content.open)\r\n    Stream.continually(zis.getNextEntry)\r\n          .takeWhile(_ != null)\r\n          .flatMap { _ =>\r\n              val br = new BufferedReader(new InputStreamReader(zis))\r\n              Stream.continually(br.readLine()).takeWhile(_ != null)\r\n          }\r\n  }\r\nval Mentions_tmp = Mentions_RDD.map(x => x.split(\"\\t\")).toDF()   \r\n\r\n\r\n// EVENTS: Données en anglais + translingual\r\nval Events_RDD = sc.binaryFiles(\"s3://vincent-partimbene-telecom-gdelt2020//20200615*.export.CSV.zip\")\r\n   .flatMap { case (name: String, content: PortableDataStream) =>\r\n    val zis = new ZipInputStream(content.open)\r\n    Stream.continually(zis.getNextEntry)\r\n          .takeWhile(_ != null)\r\n          .flatMap { _ =>\r\n              val br = new BufferedReader(new InputStreamReader(zis))\r\n              Stream.continually(br.readLine()).takeWhile(_ != null)\r\n          }\r\n  }\r\nval Events_tmp = Events_RDD.map(x => x.split(\"\\t\")).toDF()\r\n\r\n\r\n// GKG : Données en anglais\r\nval Gkg_RDD = sc.binaryFiles(\"s3://vincent-partimbene-telecom-gdelt2020//20200615*.gkg.csv.zip\").\r\n  flatMap {  // decompresser les fichiers\r\n       case (name: String, content: PortableDataStream) =>\r\n          val zis = new ZipInputStream(content.open)\r\n          Stream.continually(zis.getNextEntry).\r\n                takeWhile{case null => zis.close(); false\r\n                       case _ => true}.\r\n                flatMap { _ =>\r\n                    val br = new BufferedReader(new InputStreamReader(zis))\r\n                    Stream.continually(br.readLine()).takeWhile(_ != null)\r\n                }\r\n    }\r\nval Gkg_tmp = Gkg_RDD.map(_.split(\"\\t\")).toDF()","user":"anonymous","dateUpdated":"2021-01-18T16:56:24+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Mentions_RDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[80] at flatMap at <console>:161\nMentions_tmp: org.apache.spark.sql.DataFrame = [value: array<string>]\nEvents_RDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[83] at flatMap at <console>:175\nEvents_tmp: org.apache.spark.sql.DataFrame = [value: array<string>]\nGkg_RDD: org.apache.spark.rdd.RDD[String] = MapPartitionsRDD[86] at flatMap at <console>:189\nGkg_tmp: org.apache.spark.sql.DataFrame = [value: array<string>]\n"}]},"apps":[],"jobName":"paragraph_1610983576757_1956550723","id":"20210116-150639_1909775654","dateCreated":"2021-01-18T15:26:16+0000","dateStarted":"2021-01-18T16:56:24+0000","dateFinished":"2021-01-18T16:56:26+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11678"},{"text":"Gkg_tmp.count()","user":"anonymous","dateUpdated":"2021-01-18T15:34:17+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-94-62.ec2.internal:4040/jobs/job?id=0","http://ip-172-31-94-62.ec2.internal:4040/jobs/job?id=1"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1610983576757_-1895797836","id":"20210116-225644_911495119","dateCreated":"2021-01-18T15:26:16+0000","dateStarted":"2021-01-18T15:34:17+0000","dateFinished":"2021-01-18T15:49:05+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11679"},{"text":"Gkg_tmp.show()","user":"anonymous","dateUpdated":"2021-01-18T15:49:35+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-94-62.ec2.internal:4040/jobs/job?id=2"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1610983576758_1539108163","id":"20210118-131832_173917628","dateCreated":"2021-01-18T15:26:16+0000","dateStarted":"2021-01-18T15:49:35+0000","dateFinished":"2021-01-18T15:49:36+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11680"},{"text":"// La Regex \"/home/volume/Fichiers_gdelt/20200615/*.gkg.csv.zip\" permet de lire à la fois .gkg.csv.zip et .translation.gkg.csv.zip\r\n\r\n\r\n\r\n/*// MENTIONS : Translingual\r\nval Mentions_translation_RDD = sc.binaryFiles(\"/home/volume/Fichiers_gdelt/20200615/20200615[0-9]*.translation.mentions.CSV.zip\").\r\n   flatMap {  // decompresser les fichiers\r\n       case (name: String, content: PortableDataStream) =>\r\n          val zis = new ZipInputStream(content.open)\r\n          Stream.continually(zis.getNextEntry).\r\n                takeWhile{case null => zis.close(); false\r\n                       case _ => true}.\r\n                flatMap { _ =>\r\n                    val br = new BufferedReader(new InputStreamReader(zis))\r\n                    Stream.continually(br.readLine()).takeWhile(_ != null)\r\n                }\r\n    }\r\nval Mentions_translation_tmp = Mentions_translation_RDD.map(x => x.split(\"\\t\")).toDF()\r\n\r\n\r\n// EVENTS : Translingual\r\nval Events_translation_RDD = sc.binaryFiles(\"/home/volume/Fichiers_gdelt/20200615/20200615[0-9]*.translation.export.CSV.zip\").\r\n   flatMap {  // decompresser les fichiers\r\n       case (name: String, content: PortableDataStream) =>\r\n          val zis = new ZipInputStream(content.open)\r\n          Stream.continually(zis.getNextEntry).\r\n                takeWhile{case null => zis.close(); false\r\n                       case _ => true}.\r\n                flatMap { _ =>\r\n                    val br = new BufferedReader(new InputStreamReader(zis))\r\n                    Stream.continually(br.readLine()).takeWhile(_ != null)\r\n                }\r\n    }\r\nval Events_translation_tmp = Events_translation_RDD.map(_.split(\"\\t\")).toDF()\r\n\r\n\r\n// GKG : Translingual\r\nval Gkg_translation_RDD = sc.binaryFiles(\"/home/volume/Fichiers_gdelt/20200615/20200615[0-9]*.translation.gkg.csv.zip\")\r\n  .flatMap { case (name: String, content: PortableDataStream) =>\r\n    val zis = new ZipInputStream(content.open)\r\n    Stream.continually(zis.getNextEntry)\r\n          .takeWhile(_ != null)\r\n          .flatMap { _ =>\r\n              val br = new BufferedReader(new InputStreamReader(zis))\r\n              Stream.continually(br.readLine()).takeWhile(_ != null)\r\n          }\r\n  }\r\nval Gkg_translation_tmp = Gkg_translation_RDD.map(_.split(\"\\t\")).toDF()''' */","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576758_-1461441137","id":"20210116-175604_1216279188","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11681"},{"text":"/* Il n'y a plus de besoin d'union des dataframes. On lit un RDD unique avec l'ensemble des document par type de fichier\n\n// Union des datasets en anglais et autres langues :\nval Events_DF_array = Events_tmp.union(Events_translation_tmp)\n\nval Mentions_DF_array = Mentions_tmp.union(Mentions_translation_tmp)\n\nval Gkg_DF_array =  Gkg_tmp.union(Gkg_translation_tmp) */","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576759_-375779119","id":"20210116-220945_1593410971","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11682"},{"text":"// Utilisation du fichier Excel pour assigner le numéro aux colonnes pour trouver plus facilement + utilisation doc. Voir s'il ne faut pas plutôt mettre le nom de la colonne Big Query\n\n// Champs de la table events récupérés :\nval Events_DF_bis = Events_tmp.select(\n    $\"value\".getItem(0).as(\"Globaleventid\"),\n    $\"value\".getItem(1).as(\"Day\"),\n    $\"value\".getItem(2).as(\"MonthYear\"),\n    $\"value\".getItem(3).as(\"Year\"),\n    $\"value\".getItem(31).as(\"NumMention\"),\n    $\"value\".getItem(53).as(\"ActionGeo_CountryCode\"),\n    $\"value\".getItem(60).as(\"SourceUrl\")\n    )\n    \n    \n// Champs de la table Mentions récupérés :\nval Mentions_DF_bis = Mentions_tmp.select(\n    $\"value\".getItem(0).as(\"Globaleventid\"),\n    $\"value\".getItem(1).as(\"EventTimeDate\"),\n    $\"value\".getItem(2).as(\"MentionTimeDate\"),\n    $\"value\".getItem(5).as(\"MentionIdentifier\"),\n    $\"value\".getItem(14).as(\"ArticleLanguage\")\n    )\n\n    \n// Champs de la table Gkg récupérés :  \nval Gkg_DF_bis = Gkg_tmp.select(\n    $\"value\".getItem(0).as(\"GkgRecordId\"), \n    //$\"value\".getItem(1).as(\"DATE\"),\n    $\"value\".getItem(3).as(\"SourceCommonName\"),\n    $\"value\".getItem(4).as(\"DocumentIdentifier\"),\n    $\"value\".getItem(7).as(\"Themes\"),\n    $\"value\".getItem(9).as(\"Locations\"),\n    $\"value\".getItem(11).as(\"Persons\"),\n    $\"value\".getItem(15).as(\"Tone\")\n    )\n   \n","user":"anonymous","dateUpdated":"2021-01-18T16:56:32+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Events_DF_bis: org.apache.spark.sql.DataFrame = [Globaleventid: string, Day: string ... 5 more fields]\nMentions_DF_bis: org.apache.spark.sql.DataFrame = [Globaleventid: string, EventTimeDate: string ... 3 more fields]\nGkg_DF_bis: org.apache.spark.sql.DataFrame = [GkgRecordId: string, SourceCommonName: string ... 5 more fields]\n"}]},"apps":[],"jobName":"paragraph_1610983576759_-1441824350","id":"20210116-165608_64288513","dateCreated":"2021-01-18T15:26:16+0000","dateStarted":"2021-01-18T16:56:32+0000","dateFinished":"2021-01-18T16:56:33+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11683"},{"text":"// Preprocessing de la colonne Day, MonthYear et NumMention pour Events      \nval Events_DF = Events_DF_bis.withColumn(\"Day\", substring(col(\"Day\"), 7 , 2)).withColumn(\"MonthYear\", substring(col(\"MonthYear\"), 5 , 2)).withColumn(\"NumMention\", Events_DF_bis(\"NumMention\").cast(\"Int\"))\n\n// Preprocessing de la colonne Language pour Mentions\nval Mentions_DF = Mentions_DF_bis.withColumn(\"ArticleLanguage\", when(col(\"ArticleLanguage\").isNull, \"eng\").otherwise(substring_index(substring_index(col(\"ArticleLanguage\"), \";\",1) , \":\", -1)))\n\n// Preprocessing de la colonne Locations et Tone pour Gkg\nval Gkg_DF_bis2 = Gkg_DF_bis.withColumn(\"Locations\", when(col(\"Locations\").isNull, \"\").otherwise(substring_index(substring_index(col(\"Locations\"), \"#\", 2) , \"#\", -1))).withColumn(\"Tone\", substring_index(col(\"Tone\"), \",\" , 1))","user":"anonymous","dateUpdated":"2021-01-18T16:56:37+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Events_DF: org.apache.spark.sql.DataFrame = [Globaleventid: string, Day: string ... 5 more fields]\nMentions_DF: org.apache.spark.sql.DataFrame = [Globaleventid: string, EventTimeDate: string ... 3 more fields]\nGkg_DF_bis2: org.apache.spark.sql.DataFrame = [GkgRecordId: string, SourceCommonName: string ... 5 more fields]\n"}]},"apps":[],"jobName":"paragraph_1610983576760_1116073387","id":"20210117-232006_410460837","dateCreated":"2021-01-18T15:26:16+0000","dateStarted":"2021-01-18T16:56:37+0000","dateFinished":"2021-01-18T16:56:38+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11684"},{"text":"// Preprocessing de la colonne Tone pour Gkg\nval Gkg_DF = Gkg_DF_bis2.withColumn(\"Tone\", Gkg_DF_bis2(\"Tone\").cast(\"float\"))","user":"anonymous","dateUpdated":"2021-01-18T16:56:42+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[{"type":"TEXT","data":"Gkg_DF: org.apache.spark.sql.DataFrame = [GkgRecordId: string, SourceCommonName: string ... 5 more fields]\n"}]},"apps":[],"jobName":"paragraph_1610983576760_943549248","id":"20210117-232658_2020609674","dateCreated":"2021-01-18T15:26:16+0000","dateStarted":"2021-01-18T16:56:42+0000","dateFinished":"2021-01-18T16:56:43+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11685"},{"text":"Gkg_DF.schema","user":"anonymous","dateUpdated":"2021-01-18T15:50:20+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576761_-2101685835","id":"20210117-232548_2070253719","dateCreated":"2021-01-18T15:26:16+0000","dateStarted":"2021-01-18T15:50:20+0000","dateFinished":"2021-01-18T15:50:21+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11686"},{"text":"spark.sql(\"SELECT substring_index('4#Minya, Al Minya, Egypt#EG#EG10#28.1099#30.7503#-290636;1#dfdffdfdf', '#', 2)\").show(false)","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576761_1160616083","id":"20210117-183055_1771593855","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11687"},{"text":"spark.sql(\"SELECT substring_index(substring_index('4#Minya, Al Minya, Egypt#EG#EG10#28.1099#30.7503#-290636;1#', '#', 2) , '#', -1)\").show(false)","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576762_1154810925","id":"20210117-182821_51230148","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11688"},{"text":"spark.sql(\"SELECT substring_index('srclc:spa;eng:Moses 2.1.1 / MosesCore Europarl es-en / GT-SPA 1.0', ':', -1)\").show(false)\n","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576762_1709563820","id":"20210117-160914_990087065","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11689"},{"text":"spark.sql(\"SELECT substring_index(substring_index('srclc:spa;eng:Moses 2.1.1 / MosesCore Europarl es-en / GT-SPA 1.0', ';',1) , ':', -1)\").show(false)","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576763_-1700264713","id":"20210117-162521_1966169272","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11690"},{"text":"Events_DF.show()","user":"anonymous","dateUpdated":"2021-01-18T16:18:33+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-94-62.ec2.internal:4040/jobs/job?id=8"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1610983576763_-1376307707","id":"20210116-175930_824538957","dateCreated":"2021-01-18T15:26:16+0000","dateStarted":"2021-01-18T16:18:33+0000","dateFinished":"2021-01-18T16:18:48+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11691"},{"text":"Events_DF.schema","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576764_526183445","id":"20210117-224236_1089558614","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11692"},{"text":"Mentions_DF.show()","user":"anonymous","dateUpdated":"2021-01-18T16:18:58+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-94-62.ec2.internal:4040/jobs/job?id=9"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1610983576764_79409614","id":"20210116-180056_177403654","dateCreated":"2021-01-18T15:26:16+0000","dateStarted":"2021-01-18T16:18:58+0000","dateFinished":"2021-01-18T16:19:04+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11693"},{"text":"Gkg_DF.show()","user":"anonymous","dateUpdated":"2021-01-18T16:19:08+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-94-62.ec2.internal:4040/jobs/job?id=10"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1610983576765_-1531241598","id":"20210116-152608_1698367422","dateCreated":"2021-01-18T15:26:16+0000","dateStarted":"2021-01-18T16:19:08+0000","dateFinished":"2021-01-18T16:19:14+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11694"},{"text":"// Création de vues pour Test\r\n// On crée une vue SQL temporaire pour pouvoir faire une requête\r\nGkg_DF.createOrReplaceTempView(\"Gkg\")","user":"anonymous","dateUpdated":"2021-01-18T16:56:48+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"results":{"code":"SUCCESS","msg":[]},"apps":[],"jobName":"paragraph_1610983576765_266430376","id":"20210116-180055_382323772","dateCreated":"2021-01-18T15:26:16+0000","dateStarted":"2021-01-18T16:56:48+0000","dateFinished":"2021-01-18T16:56:51+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11695"},{"text":"// On crée une vue SQL temporaire pour pouvoir faire une requête\r\nEvents_DF.createOrReplaceTempView(\"Events\")","user":"anonymous","dateUpdated":"2021-01-18T16:57:27+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576766_-1658637575","id":"20210117-153811_430805626","dateCreated":"2021-01-18T15:26:16+0000","dateStarted":"2021-01-18T16:57:28+0000","dateFinished":"2021-01-18T16:57:31+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11696","results":{"code":"SUCCESS","msg":[]}},{"text":"// On crée une vue SQL temporaire pour pouvoir faire une requête\r\nMentions_DF.createOrReplaceTempView(\"Mentions\")","user":"anonymous","dateUpdated":"2021-01-18T16:57:37+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576766_1390999505","id":"20210117-153810_1234402008","dateCreated":"2021-01-18T15:26:16+0000","dateStarted":"2021-01-18T16:57:37+0000","dateFinished":"2021-01-18T16:57:39+0000","status":"FINISHED","progressUpdateIntervalMs":500,"$$hashKey":"object:11697","results":{"code":"SUCCESS","msg":[]}},{"text":"val Question1_DF_bis = spark.sql(\"SELECT count(Events.Globaleventid) FROM Events JOIN Mentions ON Events.Globaleventid = Mentions.Globaleventid WHERE Mentions.MentionIdentifier IN (SELECT DocumentIdentifier FROM Gkg WHERE Themes LIKE '%CORONAVIRUS%') \")\r\nQuestion1_DF_bis.show()","user":"anonymous","dateUpdated":"2021-01-18T16:57:44+0000","config":{"colWidth":12,"fontSize":9,"enabled":true,"results":{},"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"editorMode":"ace/mode/scala"},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610988640071_-1244909350","id":"20210118-165040_1609446635","dateCreated":"2021-01-18T16:50:40+0000","dateStarted":"2021-01-18T16:57:44+0000","dateFinished":"2021-01-18T16:57:47+0000","status":"ERROR","progressUpdateIntervalMs":500,"$$hashKey":"object:11698","results":{"code":"ERROR","msg":[{"type":"TEXT","data":"org.apache.spark.sql.AnalysisException: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient;\n  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:127)\n  at org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:237)\n  at org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:116)\n  at org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:104)\n  at org.apache.spark.sql.internal.SharedState.isDatabaseExistent$1(SharedState.scala:157)\n  at org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:189)\n  at org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:138)\n  at org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:47)\n  at org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:47)\n  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:91)\n  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:91)\n  at org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:701)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:832)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:787)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:817)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:810)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n  at org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:71)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:30)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$applyFunctionIfChanged$1(TreeNode.scala:345)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:381)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:213)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:379)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:30)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$applyFunctionIfChanged$1(TreeNode.scala:345)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:381)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:213)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:379)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:30)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$applyFunctionIfChanged$1(TreeNode.scala:345)\n  at org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:381)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:213)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:379)\n  at org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n  at org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:30)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:810)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:756)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1$$anonfun$2.apply(RuleExecutor.scala:92)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1$$anonfun$2.apply(RuleExecutor.scala:92)\n  at org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:91)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:88)\n  at scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n  at scala.collection.immutable.List.foldLeft(List.scala:84)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:88)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:80)\n  at scala.collection.immutable.List.foreach(List.scala:392)\n  at org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:80)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:164)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$execute$1.apply(Analyzer.scala:156)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$execute$1.apply(Analyzer.scala:156)\n  at org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withLocalMetrics(Analyzer.scala:104)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:126)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:125)\n  at org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n  at org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:125)\n  at org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n  at org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n  at org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n  at org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:80)\n  at org.apache.spark.sql.SparkSession.sql(SparkSession.scala:644)\n  ... 75 elided\nCaused by: java.lang.RuntimeException: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n  at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:522)\n  at org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:185)\n  at org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:118)\n  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n  at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n  at org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:274)\n  at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:406)\n  at org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:307)\n  at org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:67)\n  at org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:66)\n  at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:238)\n  at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:238)\n  at org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:238)\n  at org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:98)\n  ... 161 more\nCaused by: java.lang.RuntimeException: Unable to instantiate org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient\n  at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1523)\n  at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:90)\n  at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:136)\n  at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:108)\n  at org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:102)\n  at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClientFactory.createMetaStoreClient(SessionHiveMetaStoreClientFactory.java:42)\n  at org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3113)\n  at org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3148)\n  at org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\n  ... 176 more\nCaused by: java.lang.reflect.InvocationTargetException: javax.jdo.JDOFatalDataStoreException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=/var/lib/hadoop/metastore/metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------\njava.sql.SQLException: Failed to create database '/var/lib/hadoop/metastore/metastore_db', see the next exception for details.\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.createDatabase(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:664)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:208)\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:369)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:398)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:295)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:262)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:77)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:137)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:596)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:574)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:627)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:464)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5814)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:201)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:90)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:136)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:108)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:102)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClientFactory.createMetaStoreClient(SessionHiveMetaStoreClientFactory.java:42)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3113)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3148)\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:185)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:118)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:274)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:406)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:307)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:67)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:66)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:238)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:238)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:238)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:98)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:237)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:116)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:104)\n\tat org.apache.spark.sql.internal.SharedState.isDatabaseExistent$1(SharedState.scala:157)\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:189)\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:138)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:47)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:47)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:91)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:91)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:701)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:832)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:787)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:817)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:810)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:71)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$applyFunctionIfChanged$1(TreeNode.scala:345)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:213)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:379)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$applyFunctionIfChanged$1(TreeNode.scala:345)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:213)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:379)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$applyFunctionIfChanged$1(TreeNode.scala:345)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:213)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:379)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:810)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:756)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1$$anonfun$2.apply(RuleExecutor.scala:92)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1$$anonfun$2.apply(RuleExecutor.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:88)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:80)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:164)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$execute$1.apply(Analyzer.scala:156)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$execute$1.apply(Analyzer.scala:156)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withLocalMetrics(Analyzer.scala:104)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:125)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:80)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:644)\n\tat <init>(<console>:153)\n\tat <init>(<console>:162)\n\tat <init>(<console>:164)\n\tat <init>(<console>:166)\n\tat <init>(<console>:168)\n\tat <init>(<console>:170)\n\tat <init>(<console>:172)\n\tat <init>(<console>:174)\n\tat <init>(<console>:176)\n\tat <init>(<console>:178)\n\tat <init>(<console>:180)\n\tat <init>(<console>:182)\n\tat <init>(<console>:184)\n\tat <init>(<console>:186)\n\tat <init>(<console>:188)\n\tat <init>(<console>:190)\n\tat <init>(<console>:192)\n\tat <init>(<console>:194)\n\tat <init>(<console>:196)\n\tat <init>(<console>:198)\n\tat <init>(<console>:200)\n\tat <init>(<console>:202)\n\tat <init>(<console>:204)\n\tat <init>(<console>:206)\n\tat <init>(<console>:208)\n\tat <init>(<console>:210)\n\tat <init>(<console>:212)\n\tat <init>(<console>:214)\n\tat <init>(<console>:216)\n\tat <init>(<console>:218)\n\tat <init>(<console>:220)\n\tat <init>(<console>:222)\n\tat <init>(<console>:224)\n\tat <init>(<console>:226)\n\tat <init>(<console>:228)\n\tat <init>(<console>:230)\n\tat <init>(<console>:232)\n\tat .<init>(<console>:236)\n\tat .<clinit>(<console>)\n\tat .$print$lzycompute(<console>:7)\n\tat .$print(<console>:6)\n\tat $print(<console>)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:793)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1054)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:645)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:644)\n\tat scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:644)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:576)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:572)\n\tat org.apache.zeppelin.spark.SparkScala211Interpreter.scalaInterpret(SparkScala211Interpreter.scala:108)\n\tat org.apache.zeppelin.spark.BaseSparkScalaInterpreter$$anonfun$_interpret$1$1.apply(BaseSparkScalaInterpreter.scala:100)\n\tat org.apache.zeppelin.spark.BaseSparkScalaInterpreter$$anonfun$_interpret$1$1.apply(BaseSparkScalaInterpreter.scala:94)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat scala.Console$.withOut(Console.scala:65)\n\tat org.apache.zeppelin.spark.BaseSparkScalaInterpreter._interpret$1(BaseSparkScalaInterpreter.scala:94)\n\tat org.apache.zeppelin.spark.BaseSparkScalaInterpreter.interpret(BaseSparkScalaInterpreter.scala:125)\n\tat org.apache.zeppelin.spark.NewSparkInterpreter.interpret(NewSparkInterpreter.java:147)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:73)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:103)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:632)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:188)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:140)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: ERROR XJ041: Failed to create database '/var/lib/hadoop/metastore/metastore_db', see the next exception for details.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)\n\t... 246 more\nCaused by: ERROR XBM0H: Directory /mnt/var/lib/hadoop/metastore/metastore_db cannot be created.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.StorageFactoryService$10.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.services.monitor.StorageFactoryService.createServiceRoot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.createPersistentService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.createPersistentService(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.createPersistentService(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.createPersistentService(Unknown Source)\n\t... 243 more\n------\n\n  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n  at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n  at org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\n  ... 184 more\nCaused by: javax.jdo.JDOFatalDataStoreException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=/var/lib/hadoop/metastore/metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------\njava.sql.SQLException: Failed to create database '/var/lib/hadoop/metastore/metastore_db', see the next exception for details.\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.createDatabase(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:664)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:208)\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:369)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:398)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:295)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:262)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:77)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:137)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:596)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:574)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:627)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:464)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5814)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:201)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:90)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:136)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:108)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:102)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClientFactory.createMetaStoreClient(SessionHiveMetaStoreClientFactory.java:42)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3113)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3148)\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:185)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:118)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:274)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:406)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:307)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:67)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:66)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:238)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:238)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:238)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:98)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:237)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:116)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:104)\n\tat org.apache.spark.sql.internal.SharedState.isDatabaseExistent$1(SharedState.scala:157)\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:189)\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:138)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:47)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:47)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:91)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:91)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:701)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:832)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:787)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:817)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:810)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:71)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$applyFunctionIfChanged$1(TreeNode.scala:345)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:213)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:379)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$applyFunctionIfChanged$1(TreeNode.scala:345)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:213)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:379)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$applyFunctionIfChanged$1(TreeNode.scala:345)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:213)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:379)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:810)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:756)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1$$anonfun$2.apply(RuleExecutor.scala:92)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1$$anonfun$2.apply(RuleExecutor.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:88)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:80)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:164)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$execute$1.apply(Analyzer.scala:156)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$execute$1.apply(Analyzer.scala:156)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withLocalMetrics(Analyzer.scala:104)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:125)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:80)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:644)\n\tat <init>(<console>:153)\n\tat <init>(<console>:162)\n\tat <init>(<console>:164)\n\tat <init>(<console>:166)\n\tat <init>(<console>:168)\n\tat <init>(<console>:170)\n\tat <init>(<console>:172)\n\tat <init>(<console>:174)\n\tat <init>(<console>:176)\n\tat <init>(<console>:178)\n\tat <init>(<console>:180)\n\tat <init>(<console>:182)\n\tat <init>(<console>:184)\n\tat <init>(<console>:186)\n\tat <init>(<console>:188)\n\tat <init>(<console>:190)\n\tat <init>(<console>:192)\n\tat <init>(<console>:194)\n\tat <init>(<console>:196)\n\tat <init>(<console>:198)\n\tat <init>(<console>:200)\n\tat <init>(<console>:202)\n\tat <init>(<console>:204)\n\tat <init>(<console>:206)\n\tat <init>(<console>:208)\n\tat <init>(<console>:210)\n\tat <init>(<console>:212)\n\tat <init>(<console>:214)\n\tat <init>(<console>:216)\n\tat <init>(<console>:218)\n\tat <init>(<console>:220)\n\tat <init>(<console>:222)\n\tat <init>(<console>:224)\n\tat <init>(<console>:226)\n\tat <init>(<console>:228)\n\tat <init>(<console>:230)\n\tat <init>(<console>:232)\n\tat .<init>(<console>:236)\n\tat .<clinit>(<console>)\n\tat .$print$lzycompute(<console>:7)\n\tat .$print(<console>:6)\n\tat $print(<console>)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:793)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1054)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:645)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:644)\n\tat scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:644)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:576)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:572)\n\tat org.apache.zeppelin.spark.SparkScala211Interpreter.scalaInterpret(SparkScala211Interpreter.scala:108)\n\tat org.apache.zeppelin.spark.BaseSparkScalaInterpreter$$anonfun$_interpret$1$1.apply(BaseSparkScalaInterpreter.scala:100)\n\tat org.apache.zeppelin.spark.BaseSparkScalaInterpreter$$anonfun$_interpret$1$1.apply(BaseSparkScalaInterpreter.scala:94)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat scala.Console$.withOut(Console.scala:65)\n\tat org.apache.zeppelin.spark.BaseSparkScalaInterpreter._interpret$1(BaseSparkScalaInterpreter.scala:94)\n\tat org.apache.zeppelin.spark.BaseSparkScalaInterpreter.interpret(BaseSparkScalaInterpreter.scala:125)\n\tat org.apache.zeppelin.spark.NewSparkInterpreter.interpret(NewSparkInterpreter.java:147)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:73)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:103)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:632)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:188)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:140)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: ERROR XJ041: Failed to create database '/var/lib/hadoop/metastore/metastore_db', see the next exception for details.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)\n\t... 246 more\nCaused by: ERROR XBM0H: Directory /mnt/var/lib/hadoop/metastore/metastore_db cannot be created.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.StorageFactoryService$10.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.services.monitor.StorageFactoryService.createServiceRoot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.createPersistentService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.createPersistentService(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.createPersistentService(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.createPersistentService(Unknown Source)\n\t... 243 more\n------\n\n  at org.datanucleus.api.jdo.NucleusJDOHelper.getJDOExceptionForNucleusException(NucleusJDOHelper.java:436)\n  at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:788)\n  at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\n  at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\n  at sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n  at sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n  at sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n  at java.lang.reflect.Method.invoke(Method.java:498)\n  at javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n  at java.security.AccessController.doPrivileged(Native Method)\n  at javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n  at javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n  at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n  at javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n  at org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:369)\n  at org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:398)\n  at org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:295)\n  at org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:262)\n  at org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:77)\n  at org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:137)\n  at org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\n  at org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\n  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:596)\n  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:574)\n  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:627)\n  at org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:464)\n  at org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\n  at org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\n  at org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5814)\n  at org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:201)\n  at org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\n  ... 189 more\nCaused by: java.sql.SQLException: Unable to open a test connection to the given database. JDBC url = jdbc:derby:;databaseName=/var/lib/hadoop/metastore/metastore_db;create=true, username = APP. Terminating connection pool (set lazyInit to true if you expect to start your database after your app). Original Exception: ------\njava.sql.SQLException: Failed to create database '/var/lib/hadoop/metastore/metastore_db', see the next exception for details.\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.createDatabase(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n\tat org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:664)\n\tat java.sql.DriverManager.getConnection(DriverManager.java:208)\n\tat com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\n\tat com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\n\tat com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\n\tat org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\n\tat org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n\tat org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n\tat org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\n\tat org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.createPersistenceManagerFactory(JDOPersistenceManagerFactory.java:333)\n\tat org.datanucleus.api.jdo.JDOPersistenceManagerFactory.getPersistenceManagerFactory(JDOPersistenceManagerFactory.java:202)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat javax.jdo.JDOHelper$16.run(JDOHelper.java:1965)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat javax.jdo.JDOHelper.invoke(JDOHelper.java:1960)\n\tat javax.jdo.JDOHelper.invokeGetPersistenceManagerFactoryOnImplementation(JDOHelper.java:1166)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:808)\n\tat javax.jdo.JDOHelper.getPersistenceManagerFactory(JDOHelper.java:701)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPMF(ObjectStore.java:369)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.getPersistenceManager(ObjectStore.java:398)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.initialize(ObjectStore.java:295)\n\tat org.apache.hadoop.hive.metastore.ObjectStore.setConf(ObjectStore.java:262)\n\tat org.apache.hadoop.util.ReflectionUtils.setConf(ReflectionUtils.java:77)\n\tat org.apache.hadoop.util.ReflectionUtils.newInstance(ReflectionUtils.java:137)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.<init>(RawStoreProxy.java:57)\n\tat org.apache.hadoop.hive.metastore.RawStoreProxy.getProxy(RawStoreProxy.java:66)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.newRawStore(HiveMetaStore.java:596)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.getMS(HiveMetaStore.java:574)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.createDefaultDB(HiveMetaStore.java:627)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore$HMSHandler.init(HiveMetaStore.java:464)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.<init>(RetryingHMSHandler.java:66)\n\tat org.apache.hadoop.hive.metastore.RetryingHMSHandler.getProxy(RetryingHMSHandler.java:72)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStore.newRetryingHMSHandler(HiveMetaStore.java:5814)\n\tat org.apache.hadoop.hive.metastore.HiveMetaStoreClient.<init>(HiveMetaStoreClient.java:201)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClient.<init>(SessionHiveMetaStoreClient.java:74)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.hadoop.hive.metastore.MetaStoreUtils.newInstance(MetaStoreUtils.java:1521)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.<init>(RetryingMetaStoreClient.java:90)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:136)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:108)\n\tat org.apache.hadoop.hive.metastore.RetryingMetaStoreClient.getProxy(RetryingMetaStoreClient.java:102)\n\tat org.apache.hadoop.hive.ql.metadata.SessionHiveMetaStoreClientFactory.createMetaStoreClient(SessionHiveMetaStoreClientFactory.java:42)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.createMetaStoreClient(Hive.java:3113)\n\tat org.apache.hadoop.hive.ql.metadata.Hive.getMSC(Hive.java:3148)\n\tat org.apache.hadoop.hive.ql.session.SessionState.start(SessionState.java:503)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.newState(HiveClientImpl.scala:185)\n\tat org.apache.spark.sql.hive.client.HiveClientImpl.<init>(HiveClientImpl.scala:118)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n\tat sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n\tat sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n\tat java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n\tat org.apache.spark.sql.hive.client.IsolatedClientLoader.createClient(IsolatedClientLoader.scala:274)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:406)\n\tat org.apache.spark.sql.hive.HiveUtils$.newClientForMetadata(HiveUtils.scala:307)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client$lzycompute(HiveExternalCatalog.scala:67)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.client(HiveExternalCatalog.scala:66)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply$mcZ$sp(HiveExternalCatalog.scala:238)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:238)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog$$anonfun$databaseExists$1.apply(HiveExternalCatalog.scala:238)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.withClient(HiveExternalCatalog.scala:98)\n\tat org.apache.spark.sql.hive.HiveExternalCatalog.databaseExists(HiveExternalCatalog.scala:237)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog$lzycompute(SharedState.scala:116)\n\tat org.apache.spark.sql.internal.SharedState.externalCatalog(SharedState.scala:104)\n\tat org.apache.spark.sql.internal.SharedState.isDatabaseExistent$1(SharedState.scala:157)\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager$lzycompute(SharedState.scala:189)\n\tat org.apache.spark.sql.internal.SharedState.globalTempViewManager(SharedState.scala:138)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:47)\n\tat org.apache.spark.sql.hive.HiveSessionStateBuilder$$anonfun$2.apply(HiveSessionStateBuilder.scala:47)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager$lzycompute(SessionCatalog.scala:91)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.globalTempViewManager(SessionCatalog.scala:91)\n\tat org.apache.spark.sql.catalyst.catalog.SessionCatalog.lookupRelation(SessionCatalog.scala:701)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.org$apache$spark$sql$catalyst$analysis$Analyzer$ResolveRelations$$lookupTableFromCatalog(Analyzer.scala:832)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.resolveRelation(Analyzer.scala:787)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:817)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$$anonfun$apply$8.applyOrElse(Analyzer.scala:810)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$apply$1.apply(AnalysisHelper.scala:90)\n\tat org.apache.spark.sql.catalyst.trees.CurrentOrigin$.withOrigin(TreeNode.scala:71)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:89)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$applyFunctionIfChanged$1(TreeNode.scala:345)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:213)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:379)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$applyFunctionIfChanged$1(TreeNode.scala:345)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:213)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:379)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1$$anonfun$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.org$apache$spark$sql$catalyst$trees$TreeNode$$applyFunctionIfChanged$1(TreeNode.scala:345)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode$$anonfun$6.apply(TreeNode.scala:381)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapProductIterator(TreeNode.scala:213)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:379)\n\tat org.apache.spark.sql.catalyst.trees.TreeNode.mapChildren(TreeNode.scala:327)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:87)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$$anonfun$resolveOperatorsUp$1.apply(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.allowInvokingTransformsInAnalyzer(AnalysisHelper.scala:194)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$class.resolveOperatorsUp(AnalysisHelper.scala:86)\n\tat org.apache.spark.sql.catalyst.plans.logical.LogicalPlan.resolveOperatorsUp(LogicalPlan.scala:30)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:810)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$ResolveRelations$.apply(Analyzer.scala:756)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1$$anonfun$2.apply(RuleExecutor.scala:92)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1$$anonfun$2.apply(RuleExecutor.scala:92)\n\tat org.apache.spark.sql.execution.QueryExecutionMetrics$.withMetrics(QueryExecutionMetrics.scala:141)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:91)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1$$anonfun$apply$1.apply(RuleExecutor.scala:88)\n\tat scala.collection.LinearSeqOptimized$class.foldLeft(LinearSeqOptimized.scala:124)\n\tat scala.collection.immutable.List.foldLeft(List.scala:84)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:88)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor$$anonfun$execute$1.apply(RuleExecutor.scala:80)\n\tat scala.collection.immutable.List.foreach(List.scala:392)\n\tat org.apache.spark.sql.catalyst.rules.RuleExecutor.execute(RuleExecutor.scala:80)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.org$apache$spark$sql$catalyst$analysis$Analyzer$$executeSameContext(Analyzer.scala:164)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$execute$1.apply(Analyzer.scala:156)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$execute$1.apply(Analyzer.scala:156)\n\tat org.apache.spark.sql.catalyst.analysis.AnalysisContext$.withLocalMetrics(Analyzer.scala:104)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.execute(Analyzer.scala:155)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:126)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer$$anonfun$executeAndCheck$1.apply(Analyzer.scala:125)\n\tat org.apache.spark.sql.catalyst.plans.logical.AnalysisHelper$.markInAnalyzer(AnalysisHelper.scala:201)\n\tat org.apache.spark.sql.catalyst.analysis.Analyzer.executeAndCheck(Analyzer.scala:125)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed$lzycompute(QueryExecution.scala:76)\n\tat org.apache.spark.sql.execution.QueryExecution.analyzed(QueryExecution.scala:74)\n\tat org.apache.spark.sql.execution.QueryExecution.assertAnalyzed(QueryExecution.scala:66)\n\tat org.apache.spark.sql.Dataset$.ofRows(Dataset.scala:80)\n\tat org.apache.spark.sql.SparkSession.sql(SparkSession.scala:644)\n\tat <init>(<console>:153)\n\tat <init>(<console>:162)\n\tat <init>(<console>:164)\n\tat <init>(<console>:166)\n\tat <init>(<console>:168)\n\tat <init>(<console>:170)\n\tat <init>(<console>:172)\n\tat <init>(<console>:174)\n\tat <init>(<console>:176)\n\tat <init>(<console>:178)\n\tat <init>(<console>:180)\n\tat <init>(<console>:182)\n\tat <init>(<console>:184)\n\tat <init>(<console>:186)\n\tat <init>(<console>:188)\n\tat <init>(<console>:190)\n\tat <init>(<console>:192)\n\tat <init>(<console>:194)\n\tat <init>(<console>:196)\n\tat <init>(<console>:198)\n\tat <init>(<console>:200)\n\tat <init>(<console>:202)\n\tat <init>(<console>:204)\n\tat <init>(<console>:206)\n\tat <init>(<console>:208)\n\tat <init>(<console>:210)\n\tat <init>(<console>:212)\n\tat <init>(<console>:214)\n\tat <init>(<console>:216)\n\tat <init>(<console>:218)\n\tat <init>(<console>:220)\n\tat <init>(<console>:222)\n\tat <init>(<console>:224)\n\tat <init>(<console>:226)\n\tat <init>(<console>:228)\n\tat <init>(<console>:230)\n\tat <init>(<console>:232)\n\tat .<init>(<console>:236)\n\tat .<clinit>(<console>)\n\tat .$print$lzycompute(<console>:7)\n\tat .$print(<console>:6)\n\tat $print(<console>)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke0(Native Method)\n\tat sun.reflect.NativeMethodAccessorImpl.invoke(NativeMethodAccessorImpl.java:62)\n\tat sun.reflect.DelegatingMethodAccessorImpl.invoke(DelegatingMethodAccessorImpl.java:43)\n\tat java.lang.reflect.Method.invoke(Method.java:498)\n\tat scala.tools.nsc.interpreter.IMain$ReadEvalPrint.call(IMain.scala:793)\n\tat scala.tools.nsc.interpreter.IMain$Request.loadAndRun(IMain.scala:1054)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:645)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest$$anonfun$loadAndRunReq$1.apply(IMain.scala:644)\n\tat scala.reflect.internal.util.ScalaClassLoader$class.asContext(ScalaClassLoader.scala:31)\n\tat scala.reflect.internal.util.AbstractFileClassLoader.asContext(AbstractFileClassLoader.scala:19)\n\tat scala.tools.nsc.interpreter.IMain$WrappedRequest.loadAndRunReq(IMain.scala:644)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:576)\n\tat scala.tools.nsc.interpreter.IMain.interpret(IMain.scala:572)\n\tat org.apache.zeppelin.spark.SparkScala211Interpreter.scalaInterpret(SparkScala211Interpreter.scala:108)\n\tat org.apache.zeppelin.spark.BaseSparkScalaInterpreter$$anonfun$_interpret$1$1.apply(BaseSparkScalaInterpreter.scala:100)\n\tat org.apache.zeppelin.spark.BaseSparkScalaInterpreter$$anonfun$_interpret$1$1.apply(BaseSparkScalaInterpreter.scala:94)\n\tat scala.util.DynamicVariable.withValue(DynamicVariable.scala:58)\n\tat scala.Console$.withOut(Console.scala:65)\n\tat org.apache.zeppelin.spark.BaseSparkScalaInterpreter._interpret$1(BaseSparkScalaInterpreter.scala:94)\n\tat org.apache.zeppelin.spark.BaseSparkScalaInterpreter.interpret(BaseSparkScalaInterpreter.scala:125)\n\tat org.apache.zeppelin.spark.NewSparkInterpreter.interpret(NewSparkInterpreter.java:147)\n\tat org.apache.zeppelin.spark.SparkInterpreter.interpret(SparkInterpreter.java:73)\n\tat org.apache.zeppelin.interpreter.LazyOpenInterpreter.interpret(LazyOpenInterpreter.java:103)\n\tat org.apache.zeppelin.interpreter.remote.RemoteInterpreterServer$InterpretJob.jobRun(RemoteInterpreterServer.java:632)\n\tat org.apache.zeppelin.scheduler.Job.run(Job.java:188)\n\tat org.apache.zeppelin.scheduler.FIFOScheduler$1.run(FIFOScheduler.java:140)\n\tat java.util.concurrent.Executors$RunnableAdapter.call(Executors.java:511)\n\tat java.util.concurrent.FutureTask.run(FutureTask.java:266)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.access$201(ScheduledThreadPoolExecutor.java:180)\n\tat java.util.concurrent.ScheduledThreadPoolExecutor$ScheduledFutureTask.run(ScheduledThreadPoolExecutor.java:293)\n\tat java.util.concurrent.ThreadPoolExecutor.runWorker(ThreadPoolExecutor.java:1149)\n\tat java.util.concurrent.ThreadPoolExecutor$Worker.run(ThreadPoolExecutor.java:624)\n\tat java.lang.Thread.run(Thread.java:748)\nCaused by: ERROR XJ041: Failed to create database '/var/lib/hadoop/metastore/metastore_db', see the next exception for details.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)\n\t... 246 more\nCaused by: ERROR XBM0H: Directory /mnt/var/lib/hadoop/metastore/metastore_db cannot be created.\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.StorageFactoryService$10.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.services.monitor.StorageFactoryService.createServiceRoot(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.BaseMonitor.createPersistentService(Unknown Source)\n\tat org.apache.derby.impl.services.monitor.FileMonitor.createPersistentService(Unknown Source)\n\tat org.apache.derby.iapi.services.monitor.Monitor.createPersistentService(Unknown Source)\n\tat org.apache.derby.impl.jdbc.EmbedConnection$5.run(Unknown Source)\n\tat java.security.AccessController.doPrivileged(Native Method)\n\tat org.apache.derby.impl.jdbc.EmbedConnection.createPersistentService(Unknown Source)\n\t... 243 more\n------\n\n  at sun.reflect.GeneratedConstructorAccessor93.newInstance(Unknown Source)\n  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n  at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n  at com.jolbox.bonecp.PoolUtil.generateSQLException(PoolUtil.java:192)\n  at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:422)\n  at com.jolbox.bonecp.BoneCPDataSource.getConnection(BoneCPDataSource.java:120)\n  at org.datanucleus.store.rdbms.ConnectionFactoryImpl$ManagedConnectionImpl.getConnection(ConnectionFactoryImpl.java:501)\n  at org.datanucleus.store.rdbms.RDBMSStoreManager.<init>(RDBMSStoreManager.java:298)\n  at sun.reflect.NativeConstructorAccessorImpl.newInstance0(Native Method)\n  at sun.reflect.NativeConstructorAccessorImpl.newInstance(NativeConstructorAccessorImpl.java:62)\n  at sun.reflect.DelegatingConstructorAccessorImpl.newInstance(DelegatingConstructorAccessorImpl.java:45)\n  at java.lang.reflect.Constructor.newInstance(Constructor.java:423)\n  at org.datanucleus.plugin.NonManagedPluginRegistry.createExecutableExtension(NonManagedPluginRegistry.java:631)\n  at org.datanucleus.plugin.PluginManager.createExecutableExtension(PluginManager.java:301)\n  at org.datanucleus.NucleusContext.createStoreManagerForProperties(NucleusContext.java:1187)\n  at org.datanucleus.NucleusContext.initialise(NucleusContext.java:356)\n  at org.datanucleus.api.jdo.JDOPersistenceManagerFactory.freezeConfiguration(JDOPersistenceManagerFactory.java:775)\n  ... 218 more\nCaused by: java.sql.SQLException: Failed to create database '/var/lib/hadoop/metastore/metastore_db', see the next exception for details.\n  at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n  at org.apache.derby.impl.jdbc.SQLExceptionFactory.getSQLException(Unknown Source)\n  at org.apache.derby.impl.jdbc.Util.seeNextException(Unknown Source)\n  at org.apache.derby.impl.jdbc.EmbedConnection.createDatabase(Unknown Source)\n  at org.apache.derby.impl.jdbc.EmbedConnection.<init>(Unknown Source)\n  at org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n  at org.apache.derby.jdbc.InternalDriver$1.run(Unknown Source)\n  at java.security.AccessController.doPrivileged(Native Method)\n  at org.apache.derby.jdbc.InternalDriver.getNewEmbedConnection(Unknown Source)\n  at org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n  at org.apache.derby.jdbc.InternalDriver.connect(Unknown Source)\n  at org.apache.derby.jdbc.AutoloadedDriver.connect(Unknown Source)\n  at java.sql.DriverManager.getConnection(DriverManager.java:664)\n  at java.sql.DriverManager.getConnection(DriverManager.java:208)\n  at com.jolbox.bonecp.BoneCP.obtainRawInternalConnection(BoneCP.java:361)\n  at com.jolbox.bonecp.BoneCP.<init>(BoneCP.java:416)\n  ... 230 more\nCaused by: org.apache.derby.iapi.error.StandardException: Failed to create database '/var/lib/hadoop/metastore/metastore_db', see the next exception for details.\n  at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n  at org.apache.derby.impl.jdbc.SQLExceptionFactory.wrapArgsForTransportAcrossDRDA(Unknown Source)\n  ... 246 more\nCaused by: org.apache.derby.iapi.error.StandardException: Directory /mnt/var/lib/hadoop/metastore/metastore_db cannot be created.\n  at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n  at org.apache.derby.iapi.error.StandardException.newException(Unknown Source)\n  at org.apache.derby.impl.services.monitor.StorageFactoryService$10.run(Unknown Source)\n  at java.security.AccessController.doPrivileged(Native Method)\n  at org.apache.derby.impl.services.monitor.StorageFactoryService.createServiceRoot(Unknown Source)\n  at org.apache.derby.impl.services.monitor.BaseMonitor.bootService(Unknown Source)\n  at org.apache.derby.impl.services.monitor.BaseMonitor.createPersistentService(Unknown Source)\n  at org.apache.derby.impl.services.monitor.FileMonitor.createPersistentService(Unknown Source)\n  at org.apache.derby.iapi.services.monitor.Monitor.createPersistentService(Unknown Source)\n  at org.apache.derby.impl.jdbc.EmbedConnection$5.run(Unknown Source)\n  at java.security.AccessController.doPrivileged(Native Method)\n  at org.apache.derby.impl.jdbc.EmbedConnection.createPersistentService(Unknown Source)\n  ... 243 more\n"}]}},{"text":"//println(Gkg_translation_tmp.count())\nprintln(Gkg_tmp.count())\nprintln(Gkg_DF.count())\n//println(2*Gkg_translation_tmp.count()+Gkg_tmp.count())","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576767_76298143","id":"20210116-224858_1233985983","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11699"},{"text":"val sqlDF = spark.sql(\"SELECT ArticleLanguage, MentionTimeDate FROM Mentions WHERE ArticleLanguage != 'eng'\")\r\nsqlDF.show()","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576767_1723464249","id":"20210117-161519_840067875","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11700"},{"text":"val sqlDF = spark.sql(\"SELECT Globaleventid FROM Events WHERE Globaleventid == 929396943\")\r\nsqlDF.show()","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576768_1885164370","id":"20210117-153457_1558674814","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11701"},{"text":"val sqlDF = spark.sql(\"SELECT Globaleventid, Language FROM Mentions WHERE Language == 'ita'\")\r\nsqlDF.show()","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576768_-1626643703","id":"20210117-152620_582048860","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11702"},{"text":"val sqlDF = spark.sql(\"SELECT count(themes) FROM Gkg WHERE themes LIKE '%CORONAVIRUS%' \")\r\nsqlDF.show()","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576769_712500229","id":"20210116-164950_283231021","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11703"},{"text":"val sqlDF = spark.sql(\"SELECT * FROM Gkg WHERE GkgRecordId LIKE '20200615041500-T0' \")\r\nsqlDF.show()","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576769_1269461039","id":"20210116-221755_282061101","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11704"},{"text":"/*\n\n(1) EVENTS : table qui liste les évènements mondiaux\n(2) MENTIONS : table qui liste les articles faisant référence à 1 évènement\n(3) GKG : graphe des évènements\n\nOn peut joindre la table (1) et la table (2) par la catégorie GlobalEventID\nOn peut joindre la table (1) et la table (3) par SOURCEURL et DocumentIdentifier\n\n*/","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576770_-1823811260","id":"20210117-004319_1382782736","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11705"},{"text":"val Join_tmp = Events_DF.join(Mentions_DF, Events_DF(\"Globaleventid\") === Mentions_DF(\"Globaleventid\"), \"left\").drop(Mentions_DF.col(\"Globaleventid\"))","user":"anonymous","dateUpdated":"2021-01-18T16:19:45+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576770_360826314","id":"20210117-111652_4552801","dateCreated":"2021-01-18T15:26:16+0000","dateStarted":"2021-01-18T16:19:45+0000","dateFinished":"2021-01-18T16:19:46+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11706"},{"text":"Join_tmp.show()","user":"anonymous","dateUpdated":"2021-01-18T16:19:56+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-94-62.ec2.internal:4040/jobs/job?id=11","http://ip-172-31-94-62.ec2.internal:4040/jobs/job?id=12"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1610983576771_-2120692794","id":"20210117-211307_1023139939","dateCreated":"2021-01-18T15:26:16+0000","dateStarted":"2021-01-18T16:19:56+0000","dateFinished":"2021-01-18T16:27:40+0000","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11707"},{"text":"Gkg_DF.count()","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576771_-354351993","id":"20210117-112853_137564471","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11708"},{"text":"Mentions_DF.count()","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576773_-1437872534","id":"20210117-112838_1353969532","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11709"},{"text":"Events_DF.count()","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576774_-1100155752","id":"20210117-112825_1240354799","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11710"},{"text":"val Join_DF = Join_tmp.join(Gkg_DF, Join_tmp(\"SourceUrl\") === Gkg_DF(\"DocumentIdentifier\"),\"left\").drop(Join_tmp.col(\"SourceUrl\"))","user":"anonymous","dateUpdated":"2021-01-18T15:51:29+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576774_-1052691966","id":"20210117-111739_689889206","dateCreated":"2021-01-18T15:26:16+0000","dateStarted":"2021-01-18T15:51:30+0000","dateFinished":"2021-01-18T15:51:31+0000","status":"FINISHED","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11711"},{"text":"Join_DF.show()","user":"anonymous","dateUpdated":"2021-01-18T15:51:49+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"runtimeInfos":{"jobUrl":{"propertyName":"jobUrl","label":"SPARK JOB","tooltip":"View in Spark web UI","group":"spark","values":["http://ip-172-31-94-62.ec2.internal:4040/jobs/job?id=3","http://ip-172-31-94-62.ec2.internal:4040/jobs/job?id=4","http://ip-172-31-94-62.ec2.internal:4040/jobs/job?id=5"],"interpreterSettingId":"spark"}},"apps":[],"jobName":"paragraph_1610983576775_856988997","id":"20210117-233233_216162226","dateCreated":"2021-01-18T15:26:16+0000","dateStarted":"2021-01-18T15:51:49+0000","dateFinished":"2021-01-18T15:57:50+0000","status":"ERROR","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11712"},{"text":"// Création de vues pour Test\r\n// On crée une vue SQL temporaire pour pouvoir faire une requête\r\nJoin_DF.createOrReplaceTempView(\"Join\")","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576775_-779933036","id":"20210117-112528_1111065437","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11713"},{"text":"val sqlDF = spark.sql(\"SELECT count(themes) FROM Gkg WHERE themes LIKE '%CORONAVIRUS%' \")\r\nsqlDF.show()","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576776_-182185433","id":"20210117-202604_1027582957","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11714"},{"text":"//Partie sur les Requêtes :","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576776_-7052364","id":"20210117-195951_1749186517","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11715"},{"text":"//Question n°1 : Afficher le nombre d’articles/évènements qui parlent de COVID qu’il y a eu pour chaque triplet (jour, pays de l’évènement, langue de l’article).\r\n\r\nval Question1_DF = spark.sql(\"SELECT Day, MonthYear, Year, ActionGeo_CountryCode as Country, ArticleLanguage, count(*) as ArticlesNumber, count(DISTINCT Globaleventid) as EventsNumber FROM Join WHERE themes LIKE '%CORONAVIRUS%' GROUP BY Day, MonthYear, Year, ActionGeo_CountryCode, ArticleLanguage ORDER BY ArticlesNumber DESC\")\r\nQuestion1_DF.show()","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576776_153870258","id":"20210117-114246_698546357","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11716"},{"text":" z.show(spark.sql(\"SELECT Day, MonthYear, Year, ActionGeo_CountryCode as Country, ArticleLanguage, count(themes) as ArticlesNumber FROM Join WHERE themes LIKE '%CORONAVIRUS%' GROUP BY Day, MonthYear, Year, ActionGeo_CountryCode, ArticleLanguage ORDER BY ArticlesNumber DESC\"))","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{"0":{"graph":{"mode":"table","height":300,"optionOpen":false,"setting":{"table":{"tableGridState":{"columns":[{"name":"Day","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"MonthYear","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"Year","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"Country","visible":true,"width":"*","sort":{"priority":0,"direction":"asc"},"filters":[{}],"pinned":""},{"name":"ArticleLanguage","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""},{"name":"ArticlesNumber","visible":true,"width":"*","sort":{},"filters":[{}],"pinned":""}],"scrollFocus":{},"selection":[],"grouping":{"grouping":[],"aggregations":[],"rowExpandedStates":{}},"treeView":{},"pagination":{"paginationCurrentPage":1,"paginationPageSize":250}},"tableColumnTypeState":{"names":{"Day":"string","MonthYear":"string","Year":"string","Country":"string","ArticleLanguage":"string","ArticlesNumber":"string"},"updated":false},"tableOptionSpecHash":"[{\"name\":\"useFilter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable filter for columns\"},{\"name\":\"showPagination\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable pagination for better navigation\"},{\"name\":\"showAggregationFooter\",\"valueType\":\"boolean\",\"defaultValue\":false,\"widget\":\"checkbox\",\"description\":\"Enable a footer for displaying aggregated values\"}]","tableOptionValue":{"useFilter":false,"showPagination":false,"showAggregationFooter":false},"updated":false,"initialized":false}},"commonSetting":{}}}},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576777_899857390","id":"20210118-140949_1856306781","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11717"},{"text":"Join_DF.getClass","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576777_1846485859","id":"20210117-203009_689103117","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11718"},{"text":"Question1_DF.getClass","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576781_-1076793169","id":"20210117-202954_368573888","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11719"},{"text":"//Question n°2 : Pour un pays donné en paramètre, affichez les évènements qui y ont eu place triées par le nombre de mentions (tri décroissant); permettez une agrégation par jour/mois/année.","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576782_1942432451","id":"20210117-114308_449974628","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11720"},{"text":"val Pays : String = \"'CH'\"","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576782_1343320986","id":"20210117-223501_762836620","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11721"},{"text":"\r\n\r\nval Question2_DF = spark.sql(s\"\"\"SELECT ActionGeo_CountryCode as Country, Globaleventid, Day, MonthYear, Year, count(Globaleventid) as Event_Mentions_Number FROM Join WHERE ActionGeo_CountryCode == $Pays GROUP BY ActionGeo_CountryCode,  Globaleventid, Day, MonthYear, Year ORDER BY Event_Mentions_Number DESC\"\"\")\r\nQuestion2_DF.show()","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576783_-821916262","id":"20210117-204627_685079659","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11722"},{"text":"\r\n\r\n\r\nval Question2_DF_bis = spark.sql(s\"\"\"SELECT ActionGeo_CountryCode as Country, Globaleventid, Day, MonthYear, Year, NumMention FROM Join WHERE ActionGeo_CountryCode == $Pays GROUP BY ActionGeo_CountryCode,  Globaleventid, Day, MonthYear, Year, NumMention ORDER BY NumMention DESC\"\"\")\r\nQuestion2_DF_bis.show()","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576783_48985786","id":"20210117-222856_845043024","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11723"},{"text":"Question2_DF_bis.schema","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576783_896506883","id":"20210117-223932_582394067","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11724"},{"text":"\n\n//Question n°3 : Pour une source de donnés passée en paramètre (gkg.SourceCommonName) affichez les thèmes, personnes, lieux dont les articles de cette sources parlent ainsi que le nombre d’articles et le ton moyen des               articles (pour chaque thème/personne/lieu); permettez une agrégation par jour/mois/année.","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576784_-1838031687","id":"20210117-200051_1774818276","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11725"},{"text":"val Source : String = \"'thejakartapost.com'\"","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576784_109096837","id":"20210117-231833_1164331537","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11726"},{"text":"\n\nval Question3_DF = spark.sql(s\"\"\"SELECT SourceCommonName, Day, MonthYear, Year, Themes, Persons, Locations, count(SourceCommonName) as ArticlesNumber, mean(Tone) as MeanToneArticles FROM Join WHERE SourceCommonName == $Source GROUP BY SourceCommonName, Day, MonthYear, Year, Themes, Persons, Locations\"\"\")\n    Question3_DF.show()","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576785_1425823632","id":"20210117-204629_601797323","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11727"},{"text":"// Création de vues pour Test\r\n// On crée une vue SQL temporaire pour pouvoir faire une requête\r\nQuestion3_DF.createOrReplaceTempView(\"Requete3\")","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576785_-1266945882","id":"20210118-145949_189919843","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11728"},{"text":"\n\nval Question3_DF = spark.sql(s\"\"\"SELECT SourceCommonName, MonthYear, Year, sum(ArticlesNumber) as ArticlesNumber, mean(MeanToneArticles) as MeanToneArticles FROM Requete3 WHERE SourceCommonName == $Source GROUP BY SourceCommonName, MonthYear, Year\"\"\")\n    Question3_DF.show()","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576786_-1639648939","id":"20210118-145949_1987184850","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11729"},{"text":"//Question n°4 : Est-ce qu’on observe des patterns dans l’evolution qui pourraient nous permettre d’identifier la prochaine vague/pandemie.","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576786_2132456011","id":"20210117-200059_1443451725","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11730"},{"text":"/*val Question2_DF = spark.sql(\"SELECT Day, MonthYear, Year, ActionGeo_CountryCode as Country, ArticleLanguage, count(themes) as ArticlesNumber FROM Join WHERE themes LIKE '%CORONAVIRUS%' GROUP BY Day, MonthYear, Year, ActionGeo_CountryCode, ArticleLanguage ORDER BY Country \")\r\nQuestion2_DF.show()*/","user":"anonymous","dateUpdated":"2021-01-18T15:26:16+0000","config":{"editorSetting":{"language":"scala","editOnDblClick":false,"completionKey":"TAB","completionSupport":true},"colWidth":12,"editorMode":"ace/mode/scala","fontSize":9,"results":{},"enabled":true},"settings":{"params":{},"forms":{}},"apps":[],"jobName":"paragraph_1610983576786_-1302765004","id":"20210117-204631_17200182","dateCreated":"2021-01-18T15:26:16+0000","status":"READY","errorMessage":"","progressUpdateIntervalMs":500,"$$hashKey":"object:11731"}],"name":"Dl_et_Preparation","id":"2FWMV57KS","noteParams":{},"noteForms":{},"angularObjects":{"md:shared_process":[],"spark:shared_process":[]},"config":{"isZeppelinNotebookCronEnable":false,"looknfeel":"default","personalizedMode":"false"},"info":{}}